# Automated Classification of Moroccan Administrative Documents

> **Multimodal AI system combining Computer Vision and Natural Language Processing**  
> Final Project - Computer Vision & NLP Course | ENSAM Rabat

[![Accuracy](https://img.shields.io/badge/Accuracy-98.5%25-success)](https://github.com/nada-alaoui-as/classification-documents-admin)
[![Python](https://img.shields.io/badge/Python-3.8+-blue)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0-orange)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green)](LICENSE)

## ğŸ¯ Project Overview

An intelligent document classification system that automatically categorizes Moroccan administrative documents into 5 classes:
- **National ID Cards (CNIE)** - Electronic identity cards with biometric data
- **Bank Statements** - Account transaction records
- **Electricity Bills** - Utility invoices (ONE, LYDEC, REDAL)
- **Water Bills** - Water consumption invoices (ONEP, RADEEMA)
- **Employment Documents** - Pay slips, work certificates, CNSS documents

**Problem Statement:** Manual document sorting in Moroccan organizations is slow (1000s of docs/day), expensive, and error-prone. This system automates classification with 98.5% accuracy, reducing manual workload by ~80%.

## ğŸ—ï¸ System Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PDF Input  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
   â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Preprocessing Pipeline    â”‚
   â”‚  (PDFâ†’Images @ 300 DPI)   â”‚
   â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
       â”‚                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Computer      â”‚   â”‚ Natural        â”‚
â”‚ Vision Module â”‚   â”‚ Language       â”‚
â”‚               â”‚   â”‚ Processing     â”‚
â”‚ ResNet50 +    â”‚   â”‚ Module         â”‚
â”‚ Structural    â”‚   â”‚                â”‚
â”‚ Features      â”‚   â”‚ Tesseract OCR +â”‚
â”‚               â”‚   â”‚ CamemBERT      â”‚
â”‚ 98.24% acc    â”‚   â”‚ 94.12% acc     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                  â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Multimodal      â”‚
         â”‚ Fusion Layer    â”‚
         â”‚ (Weighted Vote) â”‚
         â”‚ 98.5% acc       â”‚
         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Classification  â”‚
         â”‚ Result + Conf.  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”¬ Technical Approach

### 1ï¸âƒ£ Computer Vision Pipeline

**Hybrid CNN Architecture:**
- **ResNet50 Backbone** (Pre-trained on ImageNet)
  - Fine-tuned on 74 training documents
  - Feature extraction: 2048 â†’ 128 dimensions
  - Transfer learning enables high accuracy with limited data

- **Structural Feature Extraction** (9 hand-crafted features)
  - Aspect ratio detection (card vs A4 format)
  - Face detection via Haar Cascades (for ID cards)
  - Table structure detection using Hough Line Transform
  - Dominant color analysis (blue/green backgrounds)
  - Text density computation
  - Signature zone detection
  - Numeric content density

**Model Details:**
```python
Input: 224Ã—224 RGB image
â”œâ”€ ResNet50 â†’ 128 visual features
â”œâ”€ MLP(9 structural features) â†’ 32 features
â””â”€ Concat(128+32) â†’ FC(160â†’64â†’5) â†’ Softmax
```

**Performance:** 98.24% accuracy, 0.3s inference time

### 2ï¸âƒ£ Natural Language Processing Pipeline

**Stage 1: Text Extraction**
- Tesseract OCR 5.x with French language model
- PDF â†’ 300 DPI images (optimal OCR quality)
- Character Error Rate: 2-15% depending on scan quality

**Stage 2: Baseline Classifier (Keyword Matching)**
- Manual keyword dictionaries (13-15 terms/class)
- Weighted scoring system (discriminative terms: kWh Ã—3, mÂ³ Ã—3, CNSS Ã—2)
- Result: 70% accuracy, 33% confidence
- Purpose: Establishes baseline for improvement measurement

**Stage 3: Fine-tuned CamemBERT**
- **Model:** `camembert-base` (110M parameters)
- **Why CamemBERT?** French-specific BERT trained on 138GB French text (vs English BERT)
- **Training:**
  - Platform: Google Colab (T4 GPU)
  - Epochs: 10
  - Batch size: 4
  - Learning rate: 5e-5
  - Training time: 45 minutes
- **Tokenization:** SentencePiece subword (32k vocab, max 512 tokens)
- **Result:** 94.12% accuracy (+24.12 points vs baseline), 85% confidence

**Why It Works With Only 92 Documents:**  
Transfer learning! CamemBERT already understands French. We only train the final classification layer to recognize our 5 specific document categories.

### 3ï¸âƒ£ Multimodal Fusion Strategy

**Late Fusion with Confidence-Weighted Voting:**
```python
Weights: CV=40%, Structural=30%, NLP=30%

Decision Rules (priority order):
1. Perfect Agreement: CV & NLP agree + both conf > 70%
   â†’ Average confidences

2. CV Strong: CV conf > 85% + structural validation > 60%
   â†’ 0.7Ã—CV_conf + 0.3Ã—structural_score

3. NLP Strong: NLP conf > 85% + structural validation > 60%
   â†’ 0.7Ã—NLP_conf + 0.3Ã—structural_score

4. Weighted Vote: Î£(weight Ã— confidence) for each class
   â†’ Argmax
```

**Why Late Fusion?**
- Modularity: Models train independently
- Interpretability: Can inspect each modality's contribution
- Error compensation: CV corrects NLP mistakes and vice-versa

**Example:** Electricity bill misclassified as water bill by NLP (shared vocabulary: "consumption", "meter") â†’ CV 98% confident â†’ Fusion corrects to electricity bill.

## ğŸ“Š Results & Performance

| Model | Accuracy | Avg Confidence | Inference Time |
|-------|----------|----------------|----------------|
| CV (ResNet50 + Structural) | 98.24% | 85% | 0.3s |
| NLP Baseline (Keywords) | 70.00% | 33% | 0.1s |
| NLP Fine-tuned (CamemBERT) | 94.12% | 85% | 0.5s |
| **Multimodal Fusion** | **98.50%** | **88%** | **0.8s** |

**Confusion Matrix Analysis:**
- Main errors: Water bills â†” Electricity bills (similar layouts)
- Zero confusion: CNIE vs other classes (distinct visual features)

**Robustness:**
- Clean documents: 100% accuracy
- Slightly blurred: 94% accuracy
- Heavily degraded (>20% OCR error): 70% accuracy

## ğŸ› ï¸ Tech Stack

- **Deep Learning:** PyTorch 2.0, torchvision
- **NLP:** Hugging Face Transformers, CamemBERT
- **Computer Vision:** OpenCV, PIL
- **OCR:** Tesseract 5.x, pytesseract
- **PDF Processing:** pdf2image, Poppler
- **Training:** Google Colab (T4 GPU)
- **Deployment:** CPU inference (no GPU required)

## ğŸ“ Project Structure
```
classification-documents-admin/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Original PDFs (92 documents)
â”‚   â””â”€â”€ processed/              # Converted images (300 DPI)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocessing/
â”‚   â”‚   â”œâ”€â”€ pdf_to_images.py   # PDF conversion pipeline
â”‚   â”‚   â””â”€â”€ pdf_to_text.py     # OCR extraction
â”‚   â”‚
â”‚   â”œâ”€â”€ cv_module/
â”‚   â”‚   â”œâ”€â”€ hybrid_model.py    # ResNet50 + structural features
â”‚   â”‚   â”œâ”€â”€ gabarit_detector.py # Structural feature extraction
â”‚   â”‚   â”œâ”€â”€ train.py            # Training loop
â”‚   â”‚   â””â”€â”€ cv_pipeline.py      # Inference pipeline
â”‚   â”‚
â”‚   â”œâ”€â”€ nlp/
â”‚   â”‚   â”œâ”€â”€ keyword_classifier.py      # Baseline
â”‚   â”‚   â”œâ”€â”€ camembert_classifier.py    # Fine-tuned model
â”‚   â”‚   â””â”€â”€ training/
â”‚   â”‚       â””â”€â”€ camembert_finetuning.ipynb
â”‚   â”‚
â”‚   â””â”€â”€ fusion/
â”‚       â””â”€â”€ multimodal_fusion.py # Late fusion logic
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ cv/
â”‚   â”‚   â””â”€â”€ best_model.pth      # Trained CV model (100 MB)
â”‚   â””â”€â”€ nlp/
â”‚       â””â”€â”€ camembert_finetuned/ # Fine-tuned CamemBERT (440 MB)
â”‚
â”œâ”€â”€ main.py                      # Main inference script
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### Installation
```bash
# Clone repository
git clone https://github.com/nada-alaoui-as/classification-documents-admin.git
cd classification-documents-admin

# Install dependencies
pip install -r requirements.txt

# Download pre-trained models (550 MB)
# Available at: https://drive.google.com/drive/folders/1o9BVqmkTJUmcEABZhpHl4b5PYHCTaQSH
# Extract to models/ directory
```

### Usage
```python
from src.cv_module.cv_pipeline import CVPipeline
from src.nlp.nlp_pipeline import NLPPipeline
from src.fusion.multimodal_fusion import MultimodalFusion

# Initialize pipelines
cv_pipeline = CVPipeline(model_path="models/cv/best_model.pth")
nlp_pipeline = NLPPipeline(tesseract_path="path/to/tesseract")
fusion = MultimodalFusion()

# Classify document
cv_result = cv_pipeline.process_pdf("document.pdf")
nlp_result = nlp_pipeline.process_pdf("document.pdf")

final_result = fusion.fuse(
    cv_prediction=(cv_result['category'], cv_result['confidence']),
    gabarit_scores=cv_result['gabarit_scores'],
    nlp_prediction=(nlp_result['category'], nlp_result['confidence'])
)

print(f"Category: {final_result['category']}")
print(f"Confidence: {final_result['confidence']:.2%}")
```

## ğŸ“ Key Learnings & Research Insights

1. **Transfer Learning Efficacy:** Pre-trained models (ResNet50, CamemBERT) achieve 94-98% accuracy with only 92 training examplesâ€”impossible from scratch.

2. **Multimodal Complementarity:** Visual and textual modalities capture orthogonal information. Fusion provides robustness on ambiguous cases (+20% on edge cases).

3. **Structural Priors Matter:** Hand-crafted features (aspect ratios, colors) remain valuable alongside deep learning. They provide interpretability and handle distribution shifts better.

4. **OCR Bottleneck:** Text extraction quality (2-15% CER) is the main limiting factor. Future work: Document denoising, correction models.

5. **Data Efficiency:** With smart architecture choices, high performance is achievable with limited labeled dataâ€”critical for practical deployment.

## ğŸ”® Future Work

**Short-term:**
- [ ] Dataset expansion to 200+ documents per class
- [ ] Image preprocessing (denoising, deskewing, binarization)
- [ ] Multi-page document handling
- [ ] REST API deployment (Flask/FastAPI)

**Medium-term:**
- [ ] Named Entity Recognition (extract dates, amounts, names)
- [ ] Fraud detection module
- [ ] Multilingual support (Arabic, English via mBERT/XLM-R)
- [ ] Active learning pipeline for continuous improvement

**Long-term:**
- [ ] End-to-end document understanding (LayoutLM, Donut)
- [ ] Question-answering on documents
- [ ] Production deployment as SaaS platform

## ğŸ“ˆ Impact & Applications

**Potential Deployment Scenarios:**
- **Government agencies:** Automated mail sorting
- **Banks:** Customer document processing (KYC)
- **Insurance:** Claims file organization
- **Accounting firms:** Tax document categorization

**Estimated Impact:** 80% reduction in manual processing time, handling 4500+ documents/hour in batch mode.

## ğŸ‘¥ Team

- **Nada ALAOUI** - NLP Module & Multimodal Fusion
- **Salma AMAL** - Computer Vision Module & Multimodal Fusion

**Course:** Computer Vision & Natural Language Processing  
**Institution:** ENSAM Rabat
**Instructor:** Prof. CHEFIRA  
**Date:** January 2026

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) for details

## ğŸ™ Acknowledgments

- Pre-trained models: ResNet50 (He et al., 2015), CamemBERT (Martin et al., 2019)
- OCR: Tesseract (Google)
- Frameworks: PyTorch, Hugging Face Transformers

---

**â­ If you find this project useful, please consider starring the repository!**

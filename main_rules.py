"""
Classification par r√®gles logiques + fallback NLP
"""
import sys
import os
from pathlib import Path
import re
import json
from collections import Counter

sys.path.append(str(Path(__file__).parent / 'src'))

from preprocessing.pdf_to_text import PDFTextExtractor
from nlp.camembert_classifier import CamemBERTClassifier


class RuleBasedClassifier:
    """
    Classificateur bas√© sur r√®gles logiques (AND/OR/XOR)
    Court-circuite l'inf√©rence lourde quand des mots-cl√©s discriminants sont pr√©sents
    """
    
    def __init__(self):
        # Mots-cl√©s ULTRA-DISCRIMINANTS (quasi-certitude)
        self.strong_keywords = {
            'facture_electricite': {
                'critical': ['kwh', 'kilowatt', 'kw/h'],  # AND implicite (un suffit)
                'supporting': ['√©lectricit√©', 'one', 'lydec', 'redal', 'compteur √©lectrique']
            },
            'facture_eau': {
                'critical': ['m¬≥', 'm3', 'm√®tre cube', 'metre cube'],
                'supporting': ['eau', 'onep', 'radeema', 'potable', 'assainissement']
            },
            'identite': {
                'critical': ['cnie', 'carte nationale', "carte d'identit√©"],
                'supporting': ['identit√© nationale', 'n√© le', 'n√©e le', 'nationalit√© marocaine']
            },
            'releve_bancaire': {
                'critical': ['rib', 'iban'],
                'supporting': ['solde', 'd√©bit', 'cr√©dit', 'virement', 'relev√© de compte']
            },
            'document_employeur': {
                'critical': ['cnss', 'bulletin de paie', 'bulletin de salaire'],
                'supporting': ['salaire brut', 'salaire net', 'cotisations', 'employeur']
            }
        }
        
        # Mots-cl√©s faibles (n√©cessitent combinaison)
        self.weak_keywords = {
            'facture_electricite': ['facture', 'consommation', 'abonnement', 'compteur'],
            'facture_eau': ['facture', 'consommation', 'abonnement', 'compteur'],
            'identite': ['royaume', 'maroc', 'pr√©nom', 'nom'],
            'releve_bancaire': ['banque', 'compte', 'date', 'op√©ration'],
            'document_employeur': ['attestation', 'travail', 'fonction']
        }
    
    def normalize_text(self, text):
        """Normalise le texte pour matching robuste"""
        text = text.lower()
        text = text.replace('√©', 'e').replace('√®', 'e').replace('√™', 'e')
        text = text.replace('√†', 'a').replace('√¢', 'a')
        text = text.replace('√¥', 'o')
        text = text.replace('√Æ', 'i')
        text = text.replace('√π', 'u').replace('√ª', 'u')
        text = text.replace('√ß', 'c')
        return text
    
    def apply_rule_1_critical_keyword(self, text_normalized):
        """
        R√àGLE 1 (AND implicite) : Mot-cl√© critique pr√©sent
        Si un mot-cl√© ultra-discriminant est trouv√© ‚Üí Classification imm√©diate
        """
        for category, keywords in self.strong_keywords.items():
            for keyword in keywords['critical']:
                if keyword in text_normalized:
                    return {
                        'category': category,
                        'confidence': 0.95,
                        'method': 'rule_1_critical_keyword',
                        'matched_keyword': keyword
                    }
        return None
    
    def apply_rule_2_critical_and_supporting(self, text_normalized):
        """
        R√àGLE 2 (AND) : Mot-cl√© critique + mot-cl√© support
        Renforce la confiance si les deux sont pr√©sents
        """
        for category, keywords in self.strong_keywords.items():
            critical_found = any(kw in text_normalized for kw in keywords['critical'])
            supporting_found = any(kw in text_normalized for kw in keywords['supporting'])
            
            if critical_found and supporting_found:
                return {
                    'category': category,
                    'confidence': 0.98,
                    'method': 'rule_2_critical_and_supporting'
                }
        return None
    
    def apply_rule_3_multiple_supporting(self, text_normalized):
        """
        R√àGLE 3 (OR multiple) : Au moins 3 mots-cl√©s supporting
        Si 3+ mots-cl√©s support pr√©sents ‚Üí Forte probabilit√©
        """
        for category, keywords in self.strong_keywords.items():
            supporting_matches = sum(1 for kw in keywords['supporting'] if kw in text_normalized)
            
            if supporting_matches >= 3:
                return {
                    'category': category,
                    'confidence': 0.85,
                    'method': 'rule_3_multiple_supporting',
                    'matches': supporting_matches
                }
        return None
    
    def apply_rule_4_weak_combination(self, text_normalized):
        """
        R√àGLE 4 (AND combin√©) : Combinaison mots faibles
        Facture √©lectricit√© : "facture" AND "consommation" AND "compteur"
        """
        # Facture √©lectricit√©
        if ('facture' in text_normalized and 
            'consommation' in text_normalized and 
            'compteur' in text_normalized and
            'kwh' not in text_normalized and 'm3' not in text_normalized):
            # Heuristique : si "√©lectrique/√©lectricit√©" pr√©sent
            if 'electr' in text_normalized:
                return {
                    'category': 'facture_electricite',
                    'confidence': 0.70,
                    'method': 'rule_4_weak_elec'
                }
        
        # Facture eau (m√™me logique)
        if ('facture' in text_normalized and 
            'consommation' in text_normalized and 
            'compteur' in text_normalized and
            'kwh' not in text_normalized):
            if 'eau' in text_normalized or 'potable' in text_normalized:
                return {
                    'category': 'facture_eau',
                    'confidence': 0.70,
                    'method': 'rule_4_weak_eau'
                }
        
        return None
    
    def apply_rule_5_xor_disambiguation(self, text_normalized):
        """
        R√àGLE 5 (XOR) : D√©sambigu√Øsation facture eau vs √©lectricit√©
        Si les deux semblent possibles, d√©partager par mot-cl√© exclusif
        """
        has_elec_weak = any(kw in text_normalized for kw in ['electr', 'one', 'lydec'])
        has_eau_weak = any(kw in text_normalized for kw in ['eau', 'onep', 'radeema'])
        
        # XOR : exactement un des deux
        if has_elec_weak and not has_eau_weak:
            return {
                'category': 'facture_electricite',
                'confidence': 0.75,
                'method': 'rule_5_xor_elec'
            }
        elif has_eau_weak and not has_elec_weak:
            return {
                'category': 'facture_eau',
                'confidence': 0.75,
                'method': 'rule_5_xor_eau'
            }
        
        return None
    
    def apply_rule_6_negative_exclusion(self, text_normalized):
        """
        R√àGLE 6 (AND NOT) : Exclusion par mots-cl√©s contradictoires
        Si "kwh" pr√©sent ‚Üí NE PEUT PAS √™tre facture eau
        """
        if 'kwh' in text_normalized:
            # Exclure facture_eau, releve, identite, employeur
            return {
                'category': 'facture_electricite',
                'confidence': 0.90,
                'method': 'rule_6_exclusion_kwh'
            }
        
        if 'm3' in text_normalized or 'metre cube' in text_normalized:
            return {
                'category': 'facture_eau',
                'confidence': 0.90,
                'method': 'rule_6_exclusion_m3'
            }
        
        return None
    
    def classify(self, text):
        """
        Applique les r√®gles en cascade (par ordre de priorit√©)
        Retourne d√®s qu'une r√®gle match
        """
        text_normalized = self.normalize_text(text)
        
        # Ordre de priorit√© d√©croissant
        rules = [
            self.apply_rule_1_critical_keyword,
            self.apply_rule_2_critical_and_supporting,
            self.apply_rule_6_negative_exclusion,
            self.apply_rule_3_multiple_supporting,
            self.apply_rule_5_xor_disambiguation,
            self.apply_rule_4_weak_combination
        ]
        
        for rule in rules:
            result = rule(text_normalized)
            if result:
                return result
        
        # Aucune r√®gle ne match ‚Üí Fallback sur mod√®le
        return None


class HybridClassificationPipeline:
    """
    Pipeline hybride : R√®gles logiques + Fallback NLP
    """
    
    def __init__(self, tesseract_path, nlp_model_path="models/nlp/camembert_finetuned"):
        self.ocr = PDFTextExtractor(tesseract_path)
        self.rule_classifier = RuleBasedClassifier()
        self.nlp_classifier = CamemBERTClassifier(model_path=nlp_model_path)
        
        self.stats = {
            'total': 0,
            'rules': 0,
            'fallback_nlp': 0
        }
    
    def classify_document(self, pdf_path):
        print(f"\n{'='*80}")
        print(f"üîç CLASSIFICATION HYBRIDE: {Path(pdf_path).name}")
        print(f"{'='*80}")
        
        # √âtape 1 : OCR
        print("üìÑ Extraction texte (OCR)...")
        text = self.ocr.extract_text_from_pdf(pdf_path)
        
        if len(text) < 50:
            print("‚ö†Ô∏è  Texte trop court, pas d'OCR fiable")
            text = ""
        
        # √âtape 2 : R√®gles logiques
        print("üîß Application r√®gles logiques...")
        rule_result = self.rule_classifier.classify(text)
        
        if rule_result:
            print(f"‚úÖ R√àGLE MATCH√âE: {rule_result['method']}")
            print(f"   Cat√©gorie: {rule_result['category']}")
            print(f"   Confiance: {rule_result['confidence']:.2%}")
            
            self.stats['rules'] += 1
            
            return {
                'file': Path(pdf_path).name,
                'category': rule_result['category'],
                'confidence': rule_result['confidence'],
                'method': rule_result['method'],
                'source': 'rules'
            }
        
        # √âtape 3 : Fallback NLP
        print("ü§ñ Fallback ‚Üí CamemBERT fine-tun√©...")
        category, confidence = self.nlp_classifier.predict(text)
        
        self.stats['fallback_nlp'] += 1
        
        print(f"   Cat√©gorie: {category}")
        print(f"   Confiance: {confidence:.2%}")
        
        return {
            'file': Path(pdf_path).name,
            'category': category,
            'confidence': confidence,
            'method': 'camembert_finetuned',
            'source': 'nlp_model'
        }


def main():
    tesseract_path = r'C:\Users\alaou_5lgerz1\AppData\Local\Programs\Tesseract-OCR\tesseract.exe'
    
    pipeline = HybridClassificationPipeline(tesseract_path)
    
    input_folder = "data/raw"
    output_folder = "outputs"
    os.makedirs(output_folder, exist_ok=True)
    
    pdf_files = list(Path(input_folder).glob("*.pdf"))
    
    if not pdf_files:
        print(f"‚ùå Aucun PDF trouv√© dans {input_folder}")
        return
    
    print(f"üìÅ {len(pdf_files)} PDFs √† traiter\n")
    
    results = []
    for pdf_path in pdf_files:
        try:
            result = pipeline.classify_document(str(pdf_path))
            results.append(result)
            pipeline.stats['total'] += 1
        except Exception as e:
            print(f"‚ùå Erreur sur {pdf_path}: {e}\n")
    
    # Statistiques finales
    print(f"\n{'='*80}")
    print(f"üìä STATISTIQUES FINALES")
    print(f"{'='*80}")
    print(f"Total trait√©s: {pipeline.stats['total']}")
    print(f"R√®gles logiques: {pipeline.stats['rules']} ({pipeline.stats['rules']/pipeline.stats['total']*100:.1f}%)")
    print(f"Fallback NLP: {pipeline.stats['fallback_nlp']} ({pipeline.stats['fallback_nlp']/pipeline.stats['total']*100:.1f}%)")
    
    print(f"\nR√©partition par cat√©gorie:")
    categories = Counter(r['category'] for r in results)
    for cat, count in categories.most_common():
        print(f"   {cat}: {count}")
    
    # Sauvegarder
    with open(f"{output_folder}/results_rules.json", "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"\n‚úÖ R√©sultats sauvegard√©s dans {output_folder}/results_rules.json")


if __name__ == "__main__":
    main()